

Part1: Introduction, models 2017-2020
Introduction
	- Evaluation paradigm in QA (maybe in part II problems section?)
Neural models:
	- [[Reading Wikipedia to Answer Open-Domain Questions|DRQA]]
	- Dataset: SQuAD1/2
	- Dataset: NQ
	- Example: BM25
	- [[Dense Passage Retrieval for Open-Domain Question Answering|DPR]]
	- [[REALM_ Retrieval-Augmented Language Model Pre-Training|REALM]]
	- OneSlide: BART
	- [[Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks|RAG]]
	- [[Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering|FiD]]
Bibliography I


Part2: Modern QA, current problems
Newest models:
	- [[RocketQAv2]]
	- [[ColBERTv2]]
LM in QA:
	- OneSlide: T5
	- How much knowledge can you pack in QA
	- LM is Knowledge bases
	- [ElitePLM: An Empirical Study on General Language Ability Evaluation of Pretrained Language Models](https://aclanthology.org/2022.naacl-main.258.pdf)
	- PaLM
	- RETRO (ATLAS maybe)
	- T0
Problems in QA:
	- EfficientQA competition review
	- Papers from EMNLP about temporal and georaphical dimension
	- Answer evaluation 
	- [Challenges in Generalization in Open Domain Question Answering](https://aclanthology.org/2022.findings-naacl.155.pdf)
		- multi-task DPR
Talk about other directions:
	- Multi-hop
	- QA and reasoning(?) (commonsense qa)
	- Question Generation
	- Closed-book(?)
	- Multi-lingual QA
	- QA with KB
	- Abstractive QA
PolEval2022 task 3
Conclusion
Bibliography II