[Writing System and Speaker Metadata for 2,800+ Language Varieties](http://www.lrec-conf.org/proceedings/lrec2022/pdf/2022.lrec-1.538.pdf)

Tokenization:
[Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates](https://aclanthology.org/P18-1007.pdf)
[[Multi-view subword regularization]]
[[An Embarrassingly Simple Method to Mitigate und es ira ble Properties of Pretrained Language Model Tokenizers]]
[[CHARFORMER_ FAST CHARACTER TRANSFORMERS VIA GRADIENT-BASED SUBWORD TOKENIZATION]]


Adapters:
[MAD-X: An Adapter-Based Framework for Multi-Task Cross-Lingual Transfer](https://aclanthology.org/2020.emnlp-main.617.pdf)
[MAD-G: Multilingual Adapter Generation for Efficient Cross-Lingual Transfer](https://aclanthology.org/2021.findings-emnlp.410.pdf)
[On the Effectiveness of Adapter-based Tuning for Pretrained Language Model Adaptation](https://arxiv.org/abs/2106.03164)
[Robust Transfer Learning with Pretrained Language Models through Adapters](https://aclanthology.org/2021.acl-short.108.pdf)
[Efficient Hierarchical Domain Adaptation for Pretrained Language Models](https://aclanthology.org/2022.naacl-main.96.pdf)


[Parameter-efficient Multi-task Fine-tuning for Transformers via Shared Hypernetworks](https://aclanthology.org/2021.acl-long.47.pdf)
[Composable Sparse Fine-Tuning for Cross-Lingual Transfer](https://aclanthology.org/2022.acl-long.125.pdf)
[Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/abs/2110.04366)
[Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation](https://aclanthology.org/2022.acl-long.61.pdf)
[Rethinking embedding coupling in pre-trained language models](https://arxiv.org/abs/2010.12821)
[Measure and Improve Robustness in NLP Models: A Survey](https://aclanthology.org/2022.naacl-main.339.pdf)
[PERFECT: Prompt-free and Efficient Few-shot Learning with Language Models](https://arxiv.org/abs/2204.01172)


[Pre-training Data Quality and Quantity for a Low-Resource Language: New Corpus and BERT Models for Maltese](https://arxiv.org/abs/2205.10517)
[Clean or Annotate: How to Spend a Limited Data Collection Budget](https://aclanthology.org/2022.deeplo-1.17.pdf)


[Multilingual language model Adaptive Fine-Tuning: A Study on African Languages](https://openreview.net/forum?id=HKW4Kqf4I-9)

Graham Neubig - Talk about [AutoLEX](https://www.autolex.co/) 
Framework to extract and visualize language descriptions
This approach for automatic extraction of morphological information for different languages (from UD corpus). Useful for education